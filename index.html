
 <!DOCTYPE html>

<html><head>
<title>Hao Tang</title>

<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">

<script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>

<style type="text/css">
 @import url("http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300italic,600,600italic");


	body
	{
	font-family:"Roboto",Helvetica,Arial,sans-serif;font-size:16px;line-height:1.5;font-weight:300;
    	background-color : #CDCDCD;
	}
    	.content
	{
    		width : 900px;
    		padding : 25px 30px;
    		margin : 25px auto;
    		background-color : #fff;
    		box-shadow: 0px 0px 10px #999;
    		border-radius: 15px; 
	}	
	table
	{
		padding: 5px;
	}
	
	table.pub_table,td.pub_td1,td.pub_td2
	{
		padding: 8px;
		width: 850px;
        border-collapse: separate;
        border-spacing: 15px;
        margin-top: -5px;
	}

	td.pub_td1
	{
		width:50px;
	}
    td.pub_td1 img
    {
        height:120px;
        width: 160px;
    }
	
	div#container
	{
		margin-left: auto;
		margin-right: auto;
		width: 820px;
		text-align: left;
		position: relative;
		background-color: #FFF;
	}
	div#DocInfo
	{
		color: #1367a7;
		height: 158px;
	}
	h4,h3,h2,h1
	{
		color: #3B3B3B;
	}
	h2
	{
		font-size:130%;
	}
	p
	{
		color: #5B5B5B;
		margin-bottom: 50px;
	}
	p.caption
	{
		color: #9B9B9B;
		text-align: left;
		width: 600px;
	}
	p.caption2
	{
		color: #9B9B9B;
		text-align: left;
		width: 800px;
	}
	#header_img
	{
		position: absolute;
		top: 0px; right: 0px;
    }
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}

    #mit_logo {
        position: absolute;
        left: 646px;
        top: 14px;
        width: 200px;
        height: 20px;
    }
   
    table.pub_table tr {
        outline: thin dotted #666666;
    }
    .papericon {
        border-radius: 8px; 
        -moz-box-shadow: 3px 3px 6px #888;
        -webkit-box-shadow: 3px 3px 6px #888;
        box-shadow: 3px 3px 6px #888;
        width: 180px;
	margin-top:5px;
	margin-left:5px;
	margin-bottom:5px;
    }

     .papericon_blank {

        width: 160px;
	margin-top:5px;
	margin-left:5px;
	margin-bottom:5px;
    }

    .media {
	outline: thin dotted #666666;
 	margin-bottom: 15px;	
	margin-left:10px;
    }
    .media-body {
	margin-top:5px;
	padding-left:20px;
    }

.papers-selected h5, .papers-selected h4 { display : none; }
.papers-selected .publication { display : none; }
.paperhi-only { display : none; }
.papers-selected .paperhi { display : flex; }
.papers-selected .paperlo { display : none; }

.hidden>div {
	display:none;
}

.visible>div {
	display:block;
}
</style>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-23931362-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-23931362-2');
</script>

<script type="text/javascript">
    var myPix = new Array("img/wechat.jpg")
    function choosePic() {
        var randomNum = Math.floor(Math.random() * myPix.length);
        document.getElementById("myPicture").src = myPix[randomNum];
    };
</script>

<script>
$(document).ready(function() {
  $('.paperlo button').click(function() {
     $('.papers-container').addClass('papers-selected');
  });
  $('.paperhi button').click(function() {
     $('.papers-container').removeClass('papers-selected');
  });


	$('.text_container').addClass("hidden");

	$('.text_container').click(function() {
		var $this = $(this);

		if ($this.hasClass("hidden")) {
			$(this).removeClass("hidden").addClass("visible");
			$(this).removeClass("papericon");
		} else {
			$(this).removeClass("visible").addClass("hidden");
		}
	});


});
</script>

</head>


<body>
<div class="content">
	<div id="container">

	<table>
	<tbody><tr>
	<td><img id="myPicture" src="xxx" style="float:left; padding-right:20px" height="200px"></td>
	<script>choosePic();</script>
	<td>
	<div id="DocInfo">
		<h1>Hao Tang (唐昊)</h1>
        Ph.D. Candidate<br>
	School of Computer Science and Engineering, Nanjing University of Science and Technology<br>
		Office: Room 2046, CSE Building<br>
        Email: tanghao0918_at_njust.edu.cn<br>
        <a href="TH_CV.pdf" target="_blank" rel="external">CV</a> &bull; <a href="https://scholar.google.com/citations?hl=zh-CN&user=DZXShkoAAAAJ" target="_blank" rel="external">Google Scholar</a> &bull; <a href="https://github.com/CSer-Tang-hao" target="_blank" rel="external">Github</a><br>
	</div><br>
	</td>
	</tr>
	</tbody></table>
	<br>

	<h2>About Me</h2>
        <p style="text-align:justify";>
<!--        I am a Ph.D. student at <a href="https://imag-njust.net">Intelligent Media Analysis Group (IMAG)</a> supervised by Prof. <a href="https://imag-njust.net/jinhui-tang">Jinhui Tang</a>. During Dec. 2018 - Dec. 2019, I worked as a Research Intern at <a href="http://www.noahlab.com.hk/">HUAWEI NOAH'S ARK LAB</a>, supervised by Prof. <a href="https://scholar.google.com/citations?user=61b6eYkAAAAJ&hl=en">Qi Tian</a> (IEEE Fellow). Now, I am working closely with <a href="http://lingxixie.com/Home.html">Lingxi Xie</a> and <a href="https://imag-njust.net/xiangboshu">Xiangbo Shu</a>. My research mainly focuses on visual reasoning and its applications in action understanding. In particular, I am interested in group activity recognition, compositional action recognition, and action localization/detection.-->
    	&emsp; 	I'm Hao Tang, currently a final-year Ph.D. Candidate at Nanjing University of Science and Technology in <a href="https://imag-njust.net" target="_blank" rel="external">Intelligent Media Analysis Group (IMAG)</a>, supervised by Prof. <a href="https://scholar.google.com/citations?user=ByBLlEwAAAAJ&hl=en&oi=ao" target="_blank" rel="external">Jinhui Tang</a> and co-supervised by Prof. <a href="https://scholar.google.com/citations?user=L6J2V3sAAAAJ&hl=zh-CN" target="_blank" rel="external">Zechao Li</a>.
			Before that, I received my B.Sc. degree from Harbin Engineering University in June 2018. My primary research interests mainly focus on Deep Learning and its applications in Computer Vision and Multimedia.
			The ultimate goal of my research is to develop a machine that can learn from <strong style="color:darkblue"><i>Limited</i></strong>, <strong style="color:darkblue"><i>Dynamic</i></strong> and <strong style="color:darkblue"><i>Imperfect</i></strong> data in real-world scenes like humans.
		</p>

	<!--		<h2>Recent News</h2>-->
		<h2><span style="color:red;font-size:27px"><strong>NEWS!</strong></span></h2>
    <ul style="height: 200px;overflow-y: auto">
		<div style="text-align: justify; display: block; margin-right: auto;">
<!--		<li><img src="img/new.gif"> <strong style="color:red">I'm looking for a Postdoc position starting from Summer 2024. Please contact me if you are interested in my profile.</strong></li>-->
<!--		<li>2023/03: I was invited to be a TPC member for <strong>IEEE MIPR 2023</strong>.</li>-->
		<li>2023/01: One paper was accepted by <strong>IEEE TNNLS</strong> (JCR Q1, IF=14.26).</li>
		<li>2023/01: One paper was accepted by <strong>IEEE TCSVT</strong> (JCR Q1, IF=5.86).</li>
		<li>2022/05: One paper was accepted by <strong>Pattern Recognition</strong> (JCR Q1, IF=7.74).</li>
<!--		<li>2021/08: One paper accepted by <strong>IJCAI 2021 LTDL Workshop </strong> was awarded as the <a href="BestPaper.pdf" target="_blank" rel="external" style="color:red"><strong><u>Best Paper</u></strong></a>.</li>-->
		<li>2021/08: We won the <a href="BestPaper.pdf" target="_blank" rel="external" style="color:red"><strong><u>Best Paper Award</u></strong></a> of the LTDL Workshop in IJCAI 2021.</li>
    	<li>2021/07: Our team was granted the <a href="./img/Silver%20Award.jpg" target="_blank" rel="external" style="color:red"><strong><u>Silver Award</u></strong></a> of <strong>ICIG 2021 Challenge</strong>.<br>
			<img src="./img/award.jpg" style="height: 20px;"><strong style="color:purple"><i> Workshop: Few-Shot Learning-Based High-speed Railway Catenary Image Detection and Analysis</i></strong></li>
    	<li>2021/07: I was selected for the <strong>Excellent Ph.D. Students Sponsorship Program by NJUST</strong>.</li>
   		<li>2021/06: One paper was accepted by <strong>IJCAI 2021 LTDL Workshop</strong>.</li>
		<li>2020/08: One paper was accepted by <strong>ACM Multimedia 2020</strong>.</li>
			</div>
    </ul>

	<h2>Research Experience</h2>

	<div>
	&emsp; <strong> Singapore Management University, Singapore (Jul. 2023 - Now) </strong>
	  <a href="https://www.smu.edu.sg/" target="_blank" rel="external">
		<img border="0" src="img/SMU_logo.jpg" align="right" width="120" height="60" />
	  </a>
	<ul>
	<li>
	  Visiting Scholar, <a href="http://www.shengfenghe.com/group/" target="_blank" rel="external">SMU-VUG</a></li>
	<li>
	  Hosted by Dr. <a href="https://scholar.google.com/citations?user=rBWnK8wAAAAJ&hl=en" target="_blank" rel="external">Shengfeng He</a></li>
  </ul>
  </div>

	<div>
        &emsp; <strong> Singapore University of Technology and Design, Singapore (Nov. 2022 - Jun. 2023) </strong>
          <a href="https://www.sutd.edu.sg/" target="_blank" rel="external">
            <img border="0" src="img/SUTD_logo.jpg" align="right" width="120" height="60" />
          </a>
        <ul>
        <li>
          Visiting Scholar, <a href="https://people.sutd.edu.sg/~jun_liu/" target="_blank" rel="external">SUTD-VLG</a></li>
        <li>
          Hosted by Dr. <a href="https://sites.google.com/view/junliu021/jun-liu" target="_blank" rel="external">Jun Liu</a></li>
      </ul>
      </div>

<!--	<h2>Education</h2>-->

<!--	<div>-->
<!--        &emsp; <strong> Nanjing University of Science and Technology, China (Sep. 2018 - Now) </strong>-->
<!--          <a href="http://www.njust.edu.cn/" target="_blank" rel="external">-->
<!--            <img border="0" src="img/njust_logo.jpg" align="right" width="100" height="100" />-->
<!--          </a>-->
<!--        <ul>-->
<!--        <li>-->
<!--          Doctor of Philosophy (Ph.D.), Computer Science</li>-->
<!--        <li>-->
<!--          Supervisor: Prof. <a href="https://imag-njust.net/jinhui-tang" target="_blank" rel="external">Jinhui Tang</a></li>-->
<!--		 <li>-->
<!--			Successive Master-Doctor Program</li>-->
<!--      </ul>-->
<!--      </div>-->

<!--	<div>-->
<!--       &emsp; <strong> Harbin Engineering University, China (Sep. 2014 - Jun. 2018) </strong>-->
<!--          <a href="http://www.hrbeu.edu.cn/" target="_blank" rel="external">-->
<!--            <img border="0" src="img/heu_logo.png" align="right" width="100" height="100" />-->
<!--          </a>-->
<!--        <ul>-->
<!--        <li>-->
<!--          Bachelor of Engineering (B.E.), Automation</li>-->
<!--        <li>-->
<!--          Graduated with Excellent Thesis Award</li>-->
<!--      </ul>-->
<!--      </div>-->


    <h2>Research Interests</h2>

	<ul>
		<h6><strong style="color:#ff0000">Learning From Limited or Imperfect Data</strong></h6>
		<li> Multimedia: Zero/Few-shot Learning, Fine-Grained Visual Classification/Retrieval, ... </li>
		<li> Computer Vision: Multi-Modality Object Localization/Detection/Segmentation </li>
	</ul>


<!--<sup>&#x2709</sup>-->
<div class="papers-container papers-selected">
 	<h5 class="paperlo">All Publications<button type="button" class="ml-3 btn btn-light"> Show selected</button></h5>
	<h5 class="paperhi paperhi-only">Selected Publications<button type="button" class="ml-3 btn btn-light"> Show all</button></h5>

	<h5 class="pt-2 pb-1">2023 </h5>
	<div class="publication media">
           <div class="media-body">
			   <a href="https://ieeexplore.ieee.org/document/10038499" target="_blank" rel="external"><strong style="color:darkblue">Knowledge-Guided Semantic Transfer Network for Few-Shot Image Recognition</strong></a><br>
           Zechao Li, <strong>Hao Tang</strong>, Zhimao Peng, Guo-jun Qi, and Jinhui Tang <br>
           IEEE Transactions on Neural Networks and Learning Systems [<a href="https://github.com/CSer-Tang-hao/FS-KTN">Code</a>]<br>
	</div></div>

	<div class="publication media">
           <div class="media-body">
			   <a href="https://ieeexplore.ieee.org/document/10018260" target="_blank" rel="external"><strong style="color:darkblue">Boosting Few-shot Fine-grained Recognition with Background Suppression and Foreground Alignment</strong></a><br>
           Zican Zha, <strong>Hao Tang</strong>, Yunlian Sun, and Jinhui Tang <br>
           IEEE Transactions on Circuits and Systems for Video Technology [<a href="https://github.com/CSer-Tang-hao/BSFA-FSFG">Code</a>]<br>
	</div></div>

	<h5 class="pt-2 pb-1">2022 </h5>
	<div class="publication media paperhi">
           <div class="media-body">
			   <strong style="color:black">Complementary Triple-Decoder Network for Robust RGB-T Salient Object Detection</strong><br>
            <strong>Hao Tang</strong>, Zechao Li, Di Wang, Dong Zhang, and Jinhui Tang <br>
           Under Review, 2022 <br>
	</div></div>

	<div class="publication media">
           <div class="media-body">
			   <strong style="color:black">Erasing, Transforming, and Noising Defense Network for Occluded Person Re-Identification</strong><br>
           Neng Dong, Liyan Zhang, Shuanglin Yan, <strong>Hao Tang</strong>, and Jinhui Tang <br>
           Under Review, 2022 <br>
	</div></div>

	<div class="publication media">
           <div class="media-body">
			   <a href="https://arxiv.org/abs/2208.14365" target="_blank" rel="external"><strong style="color:darkblue">Image-Specific Information Suppression and Implicit Local Alignment for Text-based Person Search</strong></a><br>
           Shuanglin Yan, <strong>Hao Tang</strong>, Liyan Zhang, and Jinhui Tang <br>
           Under Review, 2022 <br>
	</div></div>


	<div class="publication media paperhi">
           <div class="media-body">
			   <a href="https://www.sciencedirect.com/science/article/pii/S0031320322002734" target="_blank" rel="external"><strong style="color:darkblue">Learning Attention-Guided Pyramidal Features for Few-shot Fine-grained Recognition</strong></a><br>
           <strong>Hao Tang</strong>, Chengcheng Yuan, Zechao Li and Jinhui Tang <br>
<!--			   IJCAI LTDL Workshop 2021 <strong style="color:red">(Best Paper Award)</strong><br>-->
			   IJCAI LTDL Workshop 2021 <strong style="color:red"><a href="BestPaper.pdf" target="_blank" rel="external" style="color:red"><strong>Best Paper Award</strong></a></strong><br>
			   Pattern Recognition <strong style="color:red">(Journal Version)</strong> [<a href="https://github.com/CSer-Tang-hao/AGPF-FSFG">Code</a>]<br>
	</div></div>

	<h5 class="pt-2 pb-1">2021 <font size="3px"> (* indicates <strong style="color:red">equal contributions</strong>)</font> </h5>
<!--	<div class="publication media">-->
<!--           <div class="media-body">-->
<!--			   <a href="" target="_blank" rel="external"><strong style="color:darkblue">Learning Attention-Guided Pyramidal Features for Few-shot Fine-grained Recognition</strong></a><br>-->
<!--           Chengcheng Yuan<sup>*</sup>, <strong>Hao Tang</strong><sup>*</sup>, Dong Zhang, Xinguang Xiang and Zechao Li <br>-->
<!--           IJCAI LTDL Workshop 2021 <strong style="color:red">(Oral Presentation)</strong><br>-->
<!--	</div></div>-->

	<div class="publication media">
           <div class="media-body">
			   <a href="https://ieeexplore.ieee.org/abstract/document/9506685" target="_blank" rel="external"><strong style="color:darkblue">Coupled Patch Similarity Network For One-Shot Fine-Grained Image Recognition</strong></a><br>
           Sheng Tian, <strong>Hao Tang</strong>, and Longquan Dai <br>
           IEEE ICIP 2021 [<a href="https://github.com/CSer-Tang-hao/CPSN-OSFG">Code</a>]<br>
	</div></div>

	<div class="publication media">
           <div class="media-body">
			    <a href="https://ieeexplore.ieee.org/abstract/document/9428187" target="_blank" rel="external"><strong style="color:darkblue">Learning a Tree-Structured Channel-Wise Refinement Network for Efficient Image Deraining</strong></a><br>
           Di Wang<sup>*</sup>, <strong>Hao Tang</strong><sup>*</sup>, Jinshan Pan, and Jinhui Tang <br>
           IEEE ICME 2021 [<a href="https://github.com/CSer-Tang-hao/TCRN-Deraining">Code</a>]<br>
	</div></div>


	<h5 class="pt-2 pb-1">2020</h5>
	<div class="publication media paperhi">
           <div class="media-body">
			   <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413884" target="_blank" rel="external"><strong style="color:darkblue">BlockMix: Meta Regularization and Self-Calibrated Inference for Metric-Based Meta-Learning</strong></a><br>
           <strong>Hao Tang</strong>, Zechao Li, Zhimao Peng, and Jinhui Tang <br>
           ACM Multimedia 2020 <strong style="color:red">(Oral Presentation)</strong><br>
	</div></div>

</div>


</div>
    <h2>Honors & Awards</h2>
	<div>
        <ul>
			<li>Silver Award at FSL-Based High-speed Railway Catenary Image Detection and Analysis Workshop, ICIG 2021</li>
	    	<li>Best Paper Award at Long-Tailed Distribution Learning Workshop, IJCAI 2021</li>
	    	<li>Excellent Ph.D. Students Sponsorship Program of NJUST, 2021~2023</li>
<!--	    	<li>First Prize Scholarship of Nanjing University of Science and Technology, 2018, 2019, 2020</li>-->
            <li>Excellent Bachelor Thesis & Excellent Graduate at HEU, 2018</li>
            <li>First-class Innovation Scholarship, Ministry of Industry and Information Technology of China, 2017</li>
        </ul>
	</div>

	<h2>Professional Services</h2>
	<div>
		<ul>
			<li>
				<b>Journal Reviewer</b>: </br>
				&emsp; • IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI) </br>
				&emsp; • IEEE Transactions on Image Processing (T-IP) </br>
				&emsp; • IEEE Transactions on Neural Networks and Learning Systems (T-NNLS) </br>
				&emsp; • IEEE Transactions on Multimedia (T-MM) </br>
				&emsp; • IEEE Transactions on Circuits and Systems for Video Technology (T-CSVT) </br>
				&emsp; • IEEE Transactions on Biometrics, Behavior, and Identity Science (T-BIOM) </br>
				&emsp; • Pattern Recognition (PR) </br>
				&emsp; • Information Sciences (IS) </br>
				&emsp; • The Visual Computer (TVC) </br>
				&emsp; • Image and Vision Computing (IVC) </br>
			</li>
			<li>
				<b>Conference Reviewer / Program Committee Member</b>: </br>
				&emsp; • Association for the Advancement of Artificial Intelligence (AAAI): 2021-2023 </br>
				&emsp; • ACM International Conference on Multimedia (ACM MM): 2021-2023 </br>
				&emsp; • IEEE International Conference on Multimedia Information Processing and Retrieval (MIPR): 2021-2023 </br>
			</li>
		</ul>
	</div>

	<h2>Cooperation & Communication</h2>

		<ul>
<!--			<li>-->
<!--				NJUST: <a href="https://ruiyan1995.github.io/" target="_blank" rel="external">Rui Yan</a>, <a href="https://dongzhang89.github.io/" target="_blank" rel="external">Dong Zhang</a>-->
<!--			</li>-->
<!--			<li>-->
<!--				NKU: Zhimao Peng-->
<!--			</li>-->
<!--			<li>-->
<!--				DUT: <a href="https://wdhudiekou.github.io/" target="_blank" rel="external">Di Wang</a>-->
<!--			</li>-->
			<i style="color:darkcyan">
				I'm always interested in meeting new people and hearing about potential collaborations. If you'd like to work together or get in contact with me, please email me.
			</i>
		</ul>

	</div>

</body></html>
